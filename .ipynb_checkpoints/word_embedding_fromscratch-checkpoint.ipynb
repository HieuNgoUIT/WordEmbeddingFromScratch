{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # obtains tokens with a least 1 alphabet\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping(tokens):\n",
    "    word_to_id = dict()\n",
    "    id_to_word = dict()\n",
    "    for i,word in enumerate(set(tokens)):\n",
    "        word_to_id[word] = i\n",
    "        id_to_word[i] = word\n",
    "    return word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['after', 'the', 'deduction', 'of', 'the', 'costs', 'of', 'investing', 'beating', 'the', 'stock', 'market', 'is', 'a', \"loser's\", 'game']\n"
     ]
    }
   ],
   "source": [
    "doc = \"After the deduction of the costs of investing, \" \\\n",
    "      \"beating the stock market is a loser's game.\"\n",
    "tokens = tokenize(doc)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the', 'market', 'beating', 'deduction', 'after', 'stock', 'a', \"loser's\", 'is', 'investing', 'game', 'costs', 'of'}\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "word_to_id, id_to_word = mapping(tokens)\n",
    "print(set(tokens))\n",
    "print(id_to_word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#under stand code\n",
    "# def generate_training_date(tokens, id_to_word, window_size):\n",
    "#     L = len(tokens)\n",
    "#     X, Y = [], []\n",
    "#     for i in range(L):\n",
    "#         index_before_after = list(range(max(0,i-window_size,i))) + list(range(i+1, min(i+window_size+1,L)))\n",
    "#         for j in index_before_after:\n",
    "#             X.append(id_to_word[i])\n",
    "#             Y.append(id_to_word[j])\n",
    "#     return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X,Y = generate_training_date(tokens, id_to_word, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X[:10])\n",
    "#print(\"------------------------------------------------------\")\n",
    "#print(Y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#real code\n",
    "def generate_training_date(tokens, word_to_id, window_size):\n",
    "    L = len(tokens)\n",
    "    X, Y = [], []\n",
    "    for i in range(L):\n",
    "        index_before_after = list(range(max(0, i - window_size), i)) + \\\n",
    "                             list(range(i + 1, min(i + window_size + 1,L)))\n",
    "        #print(index_before_after)\n",
    "        for j in index_before_after:\n",
    "            X.append(word_to_id[tokens[i]])\n",
    "            Y.append(word_to_id[tokens[j]])\n",
    "    X = np.array(X)\n",
    "    X = np.expand_dims(X, axis=0)\n",
    "    Y = np.array(Y)\n",
    "    Y = np.expand_dims(Y, axis=0)       \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 84)\n",
      "(1, 84)\n"
     ]
    }
   ],
   "source": [
    "X,Y = generate_training_date(tokens, word_to_id, 3)\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 84)\n",
      "(13, 84)\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(id_to_word)\n",
    "m = Y.shape[1]\n",
    "Y_one_hot = np.zeros((vocab_size,m))\n",
    "Y_one_hot[Y.flatten(), np.arange(m)] = 1\n",
    "#Y_one_hot[84 cua y flatten, 0 -> 84]\n",
    "print(Y.shape)\n",
    "print(Y_one_hot.shape)\n",
    "print(Y_one_hot[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_wrd_ebd(vocab_size, embed_size):\n",
    "    random.seed(30)\n",
    "    return np.random.randn(vocab_size, embed_size) * 0.01\n",
    "\n",
    "def initialize_dense(input_size, output_size):\n",
    "    random.seed(30)\n",
    "    return np.random.randn(input_size,output_size) * 0.01\n",
    "\n",
    "def initialize_parameters(vocab_size, embed_size):\n",
    "    embedded_layer = initialize_wrd_ebd(vocab_size, embed_size)\n",
    "    dense_layer = initialize_dense(vocab_size, embed_size)\n",
    "    \n",
    "    parameters = {}\n",
    "    parameters['emb'] = embedded_layer\n",
    "    parameters['W'] = dense_layer\n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_to_word_vecs(X, parameters):\n",
    "    m = X.shape[1]\n",
    "    embeded_matrix = parameters['emb']\n",
    "    # X shape (1, 84)\n",
    "    # embeded_matrix (13, 50)\n",
    "    word_vectors = embeded_matrix[X.flatten(),:].T\n",
    "    return word_vectors\n",
    "    #print(word_vectors.shape) 50 84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters = initialize_parameter(vocab_size, 50)\n",
    "#word_vectors = input_to_word_vecs(X,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_dense(word_vectors, parameters):\n",
    "    W = parameters['W']\n",
    "    #print(W.shape) 13 50\n",
    "    Z = np.dot(W, word_vectors)\n",
    "    #print(Z.shape) 13 84\n",
    "    return W, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear_dense(word_vectors, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Z: output out of the dense layer. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    softmax_out = np.divide(np.exp(Z), np.sum(np.exp(Z), axis=0, keepdims=True) + 0.001)\n",
    "    assert(softmax_out.shape == Z.shape)\n",
    "\n",
    "    return softmax_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foward_prop(X, parameters):\n",
    "    word_vectors = input_to_word_vecs(X, parameters)\n",
    "    W, Z = linear_dense(word_vectors, parameters)\n",
    "    softmax_out = softmax(Z)\n",
    "    \n",
    "    caches = {}\n",
    "    caches['X'] = X\n",
    "    caches['word_vec'] = word_vectors\n",
    "    caches['W'] = W\n",
    "    caches['Z'] = Z\n",
    "    return softmax_out, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramaters = initialize_parameters(vocab_size, 50)\n",
    "softmax_out, caches = foward_prop(X, paramaters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(softmax_out, Y):\n",
    "    \"\"\"\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    m = softmax_out.shape[1]\n",
    "    cost = -(1 / m) * np.sum(np.log(softmax_out[Y.flatten(), np.arange(Y.shape[1])] + 0.001))\n",
    "    return cost\n",
    "def softmax_backward(Y, softmax_out):\n",
    "    \"\"\"\n",
    "    Y: labels of training data. shape: (vocab_size, m)\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    softmax_out[Y.flatten(), np.arange(m)] -= 1.0\n",
    "    dL_dZ = softmax_out\n",
    "    \n",
    "    assert(dL_dZ.shape == softmax_out.shape)\n",
    "    return dL_dZ\n",
    "\n",
    "def dense_backward(dL_dZ, caches):\n",
    "    \"\"\"\n",
    "    dL_dZ: shape: (vocab_size, m)\n",
    "    caches: dict. results from each steps of forward propagation\n",
    "    \"\"\"\n",
    "    W = caches['W']\n",
    "    word_vec = caches['word_vec']\n",
    "    m = word_vec.shape[1]\n",
    "    \n",
    "    dL_dW = (1 / m) * np.dot(dL_dZ, word_vec.T)\n",
    "    dL_dword_vec = np.dot(W.T, dL_dZ)\n",
    "\n",
    "    assert(W.shape == dL_dW.shape)\n",
    "    assert(word_vec.shape == dL_dword_vec.shape)\n",
    "    \n",
    "    return dL_dW, dL_dword_vec\n",
    "\n",
    "def backward_propagation(Y, softmax_out, caches):\n",
    "    dL_dZ = softmax_backward(Y, softmax_out)\n",
    "    dL_dW, dL_dword_vec = dense_backward(dL_dZ, caches)\n",
    "    \n",
    "    gradients = dict()\n",
    "    gradients['dL_dZ'] = dL_dZ\n",
    "    gradients['dL_dW'] = dL_dW\n",
    "    gradients['dL_dword_vec'] = dL_dword_vec\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, caches, gradients, learning_rate):\n",
    "    vocab_size, emb_size = parameters['emb'].shape\n",
    "    inds = caches['X']\n",
    "    dL_dword_vec = gradients['dL_dword_vec']\n",
    "    m = inds.shape[-1]\n",
    "    \n",
    "    parameters['emb'][inds.flatten(), :] -= dL_dword_vec.T * learning_rate\n",
    "\n",
    "    parameters['W'] -= learning_rate * gradients['dL_dW']\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def skipgram_model_training(X, Y, vocab_size, emb_size, learning_rate, epochs, batch_size=256, parameters=None, print_cost=False, plot_cost=True):\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    \n",
    "    print(X)\n",
    "    print(X.shape)\n",
    "    print(Y)\n",
    "    print(Y.shape)\n",
    "    print(\"__________________________\")\n",
    "    \n",
    "    \n",
    "    if parameters is None:\n",
    "        parameters = initialize_parameters(vocab_size, emb_size)\n",
    "        #print(parameters['WRD_EMB'].shape)\n",
    "        #print(parameters['W'].shape)\n",
    "    \n",
    "    begin_time = datetime.now()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_cost = 0\n",
    "        batch_inds = list(range(0, m, batch_size))\n",
    "        np.random.shuffle(batch_inds)\n",
    "        for i in batch_inds:\n",
    "            X_batch = X[:, i:i+batch_size]\n",
    "            Y_batch = Y[:, i:i+batch_size]\n",
    "            \n",
    "            softmax_out, caches = foward_prop(X_batch, parameters)\n",
    "            cost = cross_entropy(softmax_out, Y_batch)\n",
    "            gradients = backward_propagation(Y_batch, softmax_out, caches)\n",
    "            update_parameters(parameters, caches, gradients, learning_rate)\n",
    "            epoch_cost += np.squeeze(cost)\n",
    "            \n",
    "        costs.append(epoch_cost)\n",
    "        if print_cost and epoch % (epochs // 500) == 0:\n",
    "            print(\"Cost after epoch {}: {}\".format(epoch, epoch_cost))\n",
    "        if epoch % (epochs // 100) == 0:\n",
    "            learning_rate *= 0.98\n",
    "    end_time = datetime.now()\n",
    "    print('training time: {}'.format(end_time - begin_time))\n",
    "            \n",
    "    if plot_cost:\n",
    "        plt.plot(np.arange(epochs), costs)\n",
    "        plt.xlabel('# of epochs')\n",
    "        plt.ylabel('cost')\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  4  4  0  0  0  0  3  3  3  3  3 12 12 12 12 12 12  0  0  0  0  0  0\n",
      "  11 11 11 11 11 11 12 12 12 12 12 12  9  9  9  9  9  9  2  2  2  2  2  2\n",
      "   0  0  0  0  0  0  5  5  5  5  5  5  1  1  1  1  1  1  8  8  8  8  8  8\n",
      "   6  6  6  6  6  7  7  7  7 10 10 10]]\n",
      "(1, 84)\n",
      "[[ 0  3 12  4  3 12  0  4  0 12  0 11  4  0  3  0 11 12  0  3 12 11 12  9\n",
      "   3 12  0 12  9  2 12  0 11  9  2  0  0 11 12  2  0  5 11 12  9  0  5  1\n",
      "  12  9  2  5  1  8  9  2  0  1  8  6  2  0  5  8  6  7  0  5  1  6  7 10\n",
      "   5  1  8  7 10  1  8  6 10  8  6  7]]\n",
      "(1, 84)\n",
      "__________________________\n",
      "Cost after epoch 0: 2.552054217552027\n",
      "Cost after epoch 10: 2.551744688236997\n",
      "Cost after epoch 20: 2.5514196732755607\n",
      "Cost after epoch 30: 2.55105829271471\n",
      "Cost after epoch 40: 2.5506393446325157\n",
      "Cost after epoch 50: 2.5501400653489283\n",
      "Cost after epoch 60: 2.5495474248195156\n",
      "Cost after epoch 70: 2.548828542217019\n",
      "Cost after epoch 80: 2.547952908714706\n",
      "Cost after epoch 90: 2.5468871880759107\n",
      "Cost after epoch 100: 2.545593451391793\n",
      "Cost after epoch 110: 2.544059240427515\n",
      "Cost after epoch 120: 2.5422211336719003\n",
      "Cost after epoch 130: 2.540024232490926\n",
      "Cost after epoch 140: 2.5374102609564986\n",
      "Cost after epoch 150: 2.534314719617879\n",
      "Cost after epoch 160: 2.5307380531266452\n",
      "Cost after epoch 170: 2.526567309637441\n",
      "Cost after epoch 180: 2.5217226438948983\n",
      "Cost after epoch 190: 2.516131157236339\n",
      "Cost after epoch 200: 2.5097242966641975\n",
      "Cost after epoch 210: 2.502582129257889\n",
      "Cost after epoch 220: 2.4945745841496514\n",
      "Cost after epoch 230: 2.485671880435214\n",
      "Cost after epoch 240: 2.4758893306955065\n",
      "Cost after epoch 250: 2.4652815983472323\n",
      "Cost after epoch 260: 2.454158259445973\n",
      "Cost after epoch 270: 2.4425001528010952\n",
      "Cost after epoch 280: 2.4304671152867194\n",
      "Cost after epoch 290: 2.41827535094061\n",
      "Cost after epoch 300: 2.4061614578837185\n",
      "Cost after epoch 310: 2.3945717582980595\n",
      "Cost after epoch 320: 2.3835145469444616\n",
      "Cost after epoch 330: 2.3731316711862216\n",
      "Cost after epoch 340: 2.3635340321604716\n",
      "Cost after epoch 350: 2.3547675622095996\n",
      "Cost after epoch 360: 2.3469477812529593\n",
      "Cost after epoch 370: 2.3398452135185157\n",
      "Cost after epoch 380: 2.333320554274532\n",
      "Cost after epoch 390: 2.3272212537548804\n",
      "Cost after epoch 400: 2.3213811590052056\n",
      "Cost after epoch 410: 2.315739179436871\n",
      "Cost after epoch 420: 2.3100568087233797\n",
      "Cost after epoch 430: 2.3042004875802826\n",
      "Cost after epoch 440: 2.2980728686972243\n",
      "Cost after epoch 450: 2.29160303249832\n",
      "Cost after epoch 460: 2.2848733119814977\n",
      "Cost after epoch 470: 2.2777661900693498\n",
      "Cost after epoch 480: 2.270269986310892\n",
      "Cost after epoch 490: 2.2624043441245525\n",
      "Cost after epoch 500: 2.254204035637194\n",
      "Cost after epoch 510: 2.2458711638747015\n",
      "Cost after epoch 520: 2.2373313492778495\n",
      "Cost after epoch 530: 2.2286260656682915\n",
      "Cost after epoch 540: 2.219816455114369\n",
      "Cost after epoch 550: 2.210964146336674\n",
      "Cost after epoch 560: 2.202288013181649\n",
      "Cost after epoch 570: 2.1936996022188664\n",
      "Cost after epoch 580: 2.1852300844277814\n",
      "Cost after epoch 590: 2.1769232471611275\n",
      "Cost after epoch 600: 2.1688172801219423\n",
      "Cost after epoch 610: 2.1610844711587354\n",
      "Cost after epoch 620: 2.1536167618621933\n",
      "Cost after epoch 630: 2.146417797455044\n",
      "Cost after epoch 640: 2.1395021383779\n",
      "Cost after epoch 650: 2.1328795252539714\n",
      "Cost after epoch 660: 2.1266668801529414\n",
      "Cost after epoch 670: 2.1207551434041605\n",
      "Cost after epoch 680: 2.1151295992283212\n",
      "Cost after epoch 690: 2.1097859888004526\n",
      "Cost after epoch 700: 2.104718063120688\n",
      "Cost after epoch 710: 2.1000025306605\n",
      "Cost after epoch 720: 2.0955460034924593\n",
      "Cost after epoch 730: 2.091329784919411\n",
      "Cost after epoch 740: 2.087344799636325\n",
      "Cost after epoch 750: 2.083581916631656\n",
      "Cost after epoch 760: 2.0800944784566076\n",
      "Cost after epoch 770: 2.0768107216600593\n",
      "Cost after epoch 780: 2.0737152929712126\n",
      "Cost after epoch 790: 2.0708004834018956\n",
      "Cost after epoch 800: 2.0680588918857943\n",
      "Cost after epoch 810: 2.065528599525186\n",
      "Cost after epoch 820: 2.063156739113843\n",
      "Cost after epoch 830: 2.0609317604425\n",
      "Cost after epoch 840: 2.0588476556064466\n",
      "Cost after epoch 850: 2.0568985894091445\n",
      "Cost after epoch 860: 2.0551107414651786\n",
      "Cost after epoch 870: 2.053445674463204\n",
      "Cost after epoch 880: 2.051894468022194\n",
      "Cost after epoch 890: 2.0504520923634315\n",
      "Cost after epoch 900: 2.0491136134028296\n",
      "Cost after epoch 910: 2.0478958507997045\n",
      "Cost after epoch 920: 2.04677135505694\n",
      "Cost after epoch 930: 2.0457331593951817\n",
      "Cost after epoch 940: 2.0447770003405137\n",
      "Cost after epoch 950: 2.0438987276479055\n",
      "Cost after epoch 960: 2.04310833813439\n",
      "Cost after epoch 970: 2.042386905354372\n",
      "Cost after epoch 980: 2.041729174018741\n",
      "Cost after epoch 990: 2.041131719535548\n",
      "Cost after epoch 1000: 2.0405912503677883\n",
      "Cost after epoch 1010: 2.040113073329006\n",
      "Cost after epoch 1020: 2.0396847895031494\n",
      "Cost after epoch 1030: 2.0393026263375695\n",
      "Cost after epoch 1040: 2.0389639773571218\n",
      "Cost after epoch 1050: 2.038666354664126\n",
      "Cost after epoch 1060: 2.0384118692508837\n",
      "Cost after epoch 1070: 2.0381929448904423\n",
      "Cost after epoch 1080: 2.0380069660881976\n",
      "Cost after epoch 1090: 2.0378519818921865\n",
      "Cost after epoch 1100: 2.037726127748988\n",
      "Cost after epoch 1110: 2.0376293046857215\n",
      "Cost after epoch 1120: 2.0375573756635967\n",
      "Cost after epoch 1130: 2.0375085519550553\n",
      "Cost after epoch 1140: 2.037481347178573\n",
      "Cost after epoch 1150: 2.0374743333833103\n",
      "Cost after epoch 1160: 2.037485902126789\n",
      "Cost after epoch 1170: 2.0375143073284887\n",
      "Cost after epoch 1180: 2.037558333261359\n",
      "Cost after epoch 1190: 2.0376168182333334\n",
      "Cost after epoch 1200: 2.0376886422049334\n",
      "Cost after epoch 1210: 2.0377712381234447\n",
      "Cost after epoch 1220: 2.037864482083189\n",
      "Cost after epoch 1230: 2.0379675625871942\n",
      "Cost after epoch 1240: 2.0380795607365\n",
      "Cost after epoch 1250: 2.038199590538181\n",
      "Cost after epoch 1260: 2.0383245720436762\n",
      "Cost after epoch 1270: 2.0384554355473115\n",
      "Cost after epoch 1280: 2.038591662164286\n",
      "Cost after epoch 1290: 2.038732528256323\n",
      "Cost after epoch 1300: 2.038877338010805\n",
      "Cost after epoch 1310: 2.039022848449884\n",
      "Cost after epoch 1320: 2.039170616719131\n",
      "Cost after epoch 1330: 2.0393203370956083\n",
      "Cost after epoch 1340: 2.0394714492815536\n",
      "Cost after epoch 1350: 2.0396234173966734\n",
      "Cost after epoch 1360: 2.039773098423132\n",
      "Cost after epoch 1370: 2.0399223549984016\n",
      "Cost after epoch 1380: 2.040071035962565\n",
      "Cost after epoch 1390: 2.0402187213698553\n",
      "Cost after epoch 1400: 2.0403650136049256\n",
      "Cost after epoch 1410: 2.040507058862149\n",
      "Cost after epoch 1420: 2.0406467903131156\n",
      "Cost after epoch 1430: 2.040784169684572\n",
      "Cost after epoch 1440: 2.0409189016885283\n",
      "Cost after epoch 1450: 2.041050712302368\n",
      "Cost after epoch 1460: 2.041177163195127\n",
      "Cost after epoch 1470: 2.0413000948229856\n",
      "Cost after epoch 1480: 2.041419553122724\n",
      "Cost after epoch 1490: 2.0415353560678082\n",
      "Cost after epoch 1500: 2.041647341971177\n",
      "Cost after epoch 1510: 2.0417535565981586\n",
      "Cost after epoch 1520: 2.0418556423224214\n",
      "Cost after epoch 1530: 2.0419537076664143\n",
      "Cost after epoch 1540: 2.042047671592728\n",
      "Cost after epoch 1550: 2.0421374715262006\n",
      "Cost after epoch 1560: 2.042221651554745\n",
      "Cost after epoch 1570: 2.0423015957248705\n",
      "Cost after epoch 1580: 2.042377454770598\n",
      "Cost after epoch 1590: 2.0424492310074003\n",
      "Cost after epoch 1600: 2.0425169417853444\n",
      "Cost after epoch 1610: 2.0425795960111928\n",
      "Cost after epoch 1620: 2.042638293703152\n",
      "Cost after epoch 1630: 2.042693205875984\n",
      "Cost after epoch 1640: 2.042744394286888\n",
      "Cost after epoch 1650: 2.0427919310252904\n",
      "Cost after epoch 1660: 2.042835221653786\n",
      "Cost after epoch 1670: 2.042875082793955\n",
      "Cost after epoch 1680: 2.0429116834105785\n",
      "Cost after epoch 1690: 2.0429451177468905\n",
      "Cost after epoch 1700: 2.042975485382595\n",
      "Cost after epoch 1710: 2.043002503616713\n",
      "Cost after epoch 1720: 2.0430267259271186\n",
      "Cost after epoch 1730: 2.0430483002011965\n",
      "Cost after epoch 1740: 2.0430673286135694\n",
      "Cost after epoch 1750: 2.043083914522671\n",
      "Cost after epoch 1760: 2.043098003163645\n",
      "Cost after epoch 1770: 2.043109918428762\n",
      "Cost after epoch 1780: 2.0431197750297416\n",
      "Cost after epoch 1790: 2.043127665647789\n",
      "Cost after epoch 1800: 2.0431336815808354\n",
      "Cost after epoch 1810: 2.0431379261399774\n",
      "Cost after epoch 1820: 2.043140534133798\n",
      "Cost after epoch 1830: 2.0431415834016975\n",
      "Cost after epoch 1840: 2.043141148785797\n",
      "Cost after epoch 1850: 2.0431393028791764\n",
      "Cost after epoch 1860: 2.0431362564586366\n",
      "Cost after epoch 1870: 2.043131996826741\n",
      "Cost after epoch 1880: 2.0431265685421907\n",
      "Cost after epoch 1890: 2.04312002830268\n",
      "Cost after epoch 1900: 2.0431124310121955\n",
      "Cost after epoch 1910: 2.043104062173589\n",
      "Cost after epoch 1920: 2.0430947999012714\n",
      "Cost after epoch 1930: 2.043084663661554\n",
      "Cost after epoch 1940: 2.0430736969415504\n",
      "Cost after epoch 1950: 2.043061942629703\n",
      "Cost after epoch 1960: 2.0430497407883275\n",
      "Cost after epoch 1970: 2.0430368932710166\n",
      "Cost after epoch 1980: 2.0430234043291082\n",
      "Cost after epoch 1990: 2.043009311307639\n",
      "Cost after epoch 2000: 2.042994652333658\n",
      "Cost after epoch 2010: 2.042979808034068\n",
      "Cost after epoch 2020: 2.042964531407322\n",
      "Cost after epoch 2030: 2.0429488207392454\n",
      "Cost after epoch 2040: 2.0429327138647917\n",
      "Cost after epoch 2050: 2.0429162505294256\n",
      "Cost after epoch 2060: 2.04289983845292\n",
      "Cost after epoch 2070: 2.0428832046741765\n",
      "Cost after epoch 2080: 2.042866348569817\n",
      "Cost after epoch 2090: 2.042849312849655\n",
      "Cost after epoch 2100: 2.0428321427114287\n",
      "Cost after epoch 2110: 2.0428152561402553\n",
      "Cost after epoch 2120: 2.0427983738691324\n",
      "Cost after epoch 2130: 2.042781500373053\n",
      "Cost after epoch 2140: 2.042764684390592\n",
      "Cost after epoch 2150: 2.0427479770449373\n",
      "Cost after epoch 2160: 2.042731784870885\n",
      "Cost after epoch 2170: 2.0427158397194067\n",
      "Cost after epoch 2180: 2.0427001520163945\n",
      "Cost after epoch 2190: 2.0426847744014998\n",
      "Cost after epoch 2200: 2.0426697611799223\n",
      "Cost after epoch 2210: 2.042655481794466\n",
      "Cost after epoch 2220: 2.0426416949841926\n",
      "Cost after epoch 2230: 2.042628415469428\n",
      "Cost after epoch 2240: 2.042615695237174\n",
      "Cost after epoch 2250: 2.042603586833872\n",
      "Cost after epoch 2260: 2.0425923960001735\n",
      "Cost after epoch 2270: 2.0425819225607587\n",
      "Cost after epoch 2280: 2.042572182524634\n",
      "Cost after epoch 2290: 2.042563221673515\n",
      "Cost after epoch 2300: 2.042555085154587\n",
      "Cost after epoch 2310: 2.0425479912931404\n",
      "Cost after epoch 2320: 2.0425417932027377\n",
      "Cost after epoch 2330: 2.0425365050582163\n",
      "Cost after epoch 2340: 2.042532161351615\n",
      "Cost after epoch 2350: 2.0425287949221915\n",
      "Cost after epoch 2360: 2.0425265194049786\n",
      "Cost after epoch 2370: 2.042525250596626\n",
      "Cost after epoch 2380: 2.0425249984982443\n",
      "Cost after epoch 2390: 2.0425257827529224\n",
      "Cost after epoch 2400: 2.042527620669874\n",
      "Cost after epoch 2410: 2.0425305126893383\n",
      "Cost after epoch 2420: 2.042534442924182\n",
      "Cost after epoch 2430: 2.0425394160363886\n",
      "Cost after epoch 2440: 2.042545435246697\n",
      "Cost after epoch 2450: 2.0425525011463566\n",
      "Cost after epoch 2460: 2.0425605020479076\n",
      "Cost after epoch 2470: 2.04256949147852\n",
      "Cost after epoch 2480: 2.0425794687096572\n",
      "Cost after epoch 2490: 2.042590420884207\n",
      "Cost after epoch 2500: 2.042602332568039\n",
      "Cost after epoch 2510: 2.0426149900104034\n",
      "Cost after epoch 2520: 2.042628512365726\n",
      "Cost after epoch 2530: 2.0426428942490658\n",
      "Cost after epoch 2540: 2.042658108587274\n",
      "Cost after epoch 2550: 2.042674126047092\n",
      "Cost after epoch 2560: 2.042690648332584\n",
      "Cost after epoch 2570: 2.042707851726357\n",
      "Cost after epoch 2580: 2.0427257272979475\n",
      "Cost after epoch 2590: 2.042744236607\n",
      "Cost after epoch 2600: 2.0427633394380074\n",
      "Cost after epoch 2610: 2.0427826758612926\n",
      "Cost after epoch 2620: 2.0428024667282805\n",
      "Cost after epoch 2630: 2.0428227006906154\n",
      "Cost after epoch 2640: 2.0428433312621466\n",
      "Cost after epoch 2650: 2.0428643107465385\n",
      "Cost after epoch 2660: 2.042885243688393\n",
      "Cost after epoch 2670: 2.042906379882067\n",
      "Cost after epoch 2680: 2.0429277064540607\n",
      "Cost after epoch 2690: 2.042949172245186\n",
      "Cost after epoch 2700: 2.042970725461061\n",
      "Cost after epoch 2710: 2.0429919623597965\n",
      "Cost after epoch 2720: 2.0430131441541435\n",
      "Cost after epoch 2730: 2.0430342569686006\n",
      "Cost after epoch 2740: 2.0430552480921107\n",
      "Cost after epoch 2750: 2.0430760647136643\n",
      "Cost after epoch 2760: 2.043096321402441\n",
      "Cost after epoch 2770: 2.043116272553645\n",
      "Cost after epoch 2780: 2.0431359034065557\n",
      "Cost after epoch 2790: 2.043155162377761\n",
      "Cost after epoch 2800: 2.0431739982480175\n",
      "Cost after epoch 2810: 2.0431920684504647\n",
      "Cost after epoch 2820: 2.043209602703602\n",
      "Cost after epoch 2830: 2.043226585104763\n",
      "Cost after epoch 2840: 2.043242967339795\n",
      "Cost after epoch 2850: 2.043258701835413\n",
      "Cost after epoch 2860: 2.043273510382428\n",
      "Cost after epoch 2870: 2.0432875814530833\n",
      "Cost after epoch 2880: 2.043300897447411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 2890: 2.043313414899356\n",
      "Cost after epoch 2900: 2.0433250913681142\n",
      "Cost after epoch 2910: 2.043335730791022\n",
      "Cost after epoch 2920: 2.0433454658132555\n",
      "Cost after epoch 2930: 2.043354276378127\n",
      "Cost after epoch 2940: 2.0433621249091045\n",
      "Cost after epoch 2950: 2.0433689750502455\n",
      "Cost after epoch 2960: 2.0433747266498274\n",
      "Cost after epoch 2970: 2.043379444410894\n",
      "Cost after epoch 2980: 2.043383104954441\n",
      "Cost after epoch 2990: 2.043385677155222\n",
      "Cost after epoch 3000: 2.043387131223503\n",
      "Cost after epoch 3010: 2.043387472610294\n",
      "Cost after epoch 3020: 2.043386688694578\n",
      "Cost after epoch 3030: 2.043384751893518\n",
      "Cost after epoch 3040: 2.043381637691055\n",
      "Cost after epoch 3050: 2.043377322952853\n",
      "Cost after epoch 3060: 2.043371924516008\n",
      "Cost after epoch 3070: 2.0433653458702197\n",
      "Cost after epoch 3080: 2.0433575544074434\n",
      "Cost after epoch 3090: 2.0433485320576414\n",
      "Cost after epoch 3100: 2.0433382621230325\n",
      "Cost after epoch 3110: 2.043326975020599\n",
      "Cost after epoch 3120: 2.043314486534694\n",
      "Cost after epoch 3130: 2.0433007583294245\n",
      "Cost after epoch 3140: 2.0432857783782983\n",
      "Cost after epoch 3150: 2.0432695359749555\n",
      "Cost after epoch 3160: 2.0432523741413644\n",
      "Cost after epoch 3170: 2.043234019706054\n",
      "Cost after epoch 3180: 2.043214428061733\n",
      "Cost after epoch 3190: 2.043193592658847\n",
      "Cost after epoch 3200: 2.0431715081853197\n",
      "Cost after epoch 3210: 2.0431486266019383\n",
      "Cost after epoch 3220: 2.043124586830941\n",
      "Cost after epoch 3230: 2.043099337632226\n",
      "Cost after epoch 3240: 2.0430728772677536\n",
      "Cost after epoch 3250: 2.043045205134413\n",
      "Cost after epoch 3260: 2.043016876271341\n",
      "Cost after epoch 3270: 2.0429874447273533\n",
      "Cost after epoch 3280: 2.042956852458108\n",
      "Cost after epoch 3290: 2.0429251018266856\n",
      "Cost after epoch 3300: 2.0428921962175624\n",
      "Cost after epoch 3310: 2.0428587861814003\n",
      "Cost after epoch 3320: 2.042824345568431\n",
      "Cost after epoch 3330: 2.0427888095440028\n",
      "Cost after epoch 3340: 2.0427521838591263\n",
      "Cost after epoch 3350: 2.0427144751687694\n",
      "Cost after epoch 3360: 2.0426764207203685\n",
      "Cost after epoch 3370: 2.042637420137726\n",
      "Cost after epoch 3380: 2.042597401981054\n",
      "Cost after epoch 3390: 2.0425563747044864\n",
      "Cost after epoch 3400: 2.0425143475501604\n",
      "Cost after epoch 3410: 2.0424721348149215\n",
      "Cost after epoch 3420: 2.042429068834048\n",
      "Cost after epoch 3430: 2.0423850718983423\n",
      "Cost after epoch 3440: 2.0423401545301196\n",
      "Cost after epoch 3450: 2.042294327929082\n",
      "Cost after epoch 3460: 2.0422484733260506\n",
      "Cost after epoch 3470: 2.0422018633645593\n",
      "Cost after epoch 3480: 2.0421544145276713\n",
      "Cost after epoch 3490: 2.0421061388341446\n",
      "Cost after epoch 3500: 2.042057048877404\n",
      "Cost after epoch 3510: 2.042008082549335\n",
      "Cost after epoch 3520: 2.041958460782759\n",
      "Cost after epoch 3530: 2.0419080948142807\n",
      "Cost after epoch 3540: 2.041856997657398\n",
      "Cost after epoch 3550: 2.041805182806972\n",
      "Cost after epoch 3560: 2.041753634642474\n",
      "Cost after epoch 3570: 2.0417015305129826\n",
      "Cost after epoch 3580: 2.0416487770432785\n",
      "Cost after epoch 3590: 2.0415953878117836\n",
      "Cost after epoch 3600: 2.041541376795104\n",
      "Cost after epoch 3610: 2.0414877649921914\n",
      "Cost after epoch 3620: 2.0414336942406073\n",
      "Cost after epoch 3630: 2.0413790672364067\n",
      "Cost after epoch 3640: 2.0413238977632746\n",
      "Cost after epoch 3650: 2.0412681999301197\n",
      "Cost after epoch 3660: 2.0412130219517395\n",
      "Cost after epoch 3670: 2.0411574780079653\n",
      "Cost after epoch 3680: 2.0411014675750745\n",
      "Cost after epoch 3690: 2.0410450043482915\n",
      "Cost after epoch 3700: 2.0409881022850156\n",
      "Cost after epoch 3710: 2.0409318279941164\n",
      "Cost after epoch 3720: 2.040875275504151\n",
      "Cost after epoch 3730: 2.040818341786593\n",
      "Cost after epoch 3740: 2.040761040214703\n",
      "Cost after epoch 3750: 2.0407033843701576\n",
      "Cost after epoch 3760: 2.0406464510970865\n",
      "Cost after epoch 3770: 2.040589321334771\n",
      "Cost after epoch 3780: 2.04053189025411\n",
      "Cost after epoch 3790: 2.040474170726553\n",
      "Cost after epoch 3800: 2.040416175786726\n",
      "Cost after epoch 3810: 2.0403589850684214\n",
      "Cost after epoch 3820: 2.040301672970214\n",
      "Cost after epoch 3830: 2.040244133542222\n",
      "Cost after epoch 3840: 2.0401863790210553\n",
      "Cost after epoch 3850: 2.0401284217689097\n",
      "Cost after epoch 3860: 2.040071337501469\n",
      "Cost after epoch 3870: 2.0400142000671666\n",
      "Cost after epoch 3880: 2.0399569030399363\n",
      "Cost after epoch 3890: 2.0398994579285823\n",
      "Cost after epoch 3900: 2.039841876336656\n",
      "Cost after epoch 3910: 2.0397852240956\n",
      "Cost after epoch 3920: 2.039728579912239\n",
      "Cost after epoch 3930: 2.0396718374857064\n",
      "Cost after epoch 3940: 2.039615007537677\n",
      "Cost after epoch 3950: 2.039558100859603\n",
      "Cost after epoch 3960: 2.039502168161499\n",
      "Cost after epoch 3970: 2.0394462978280923\n",
      "Cost after epoch 3980: 2.0393903842362633\n",
      "Cost after epoch 3990: 2.0393344372889497\n",
      "Cost after epoch 4000: 2.0392784669389403\n",
      "Cost after epoch 4010: 2.0392235042412503\n",
      "Cost after epoch 4020: 2.03916865149562\n",
      "Cost after epoch 4030: 2.039113804256903\n",
      "Cost after epoch 4040: 2.0390589716000806\n",
      "Cost after epoch 4050: 2.039004162634323\n",
      "Cost after epoch 4060: 2.03895038489497\n",
      "Cost after epoch 4070: 2.0388967582688378\n",
      "Cost after epoch 4080: 2.038843179934928\n",
      "Cost after epoch 4090: 2.038789658148717\n",
      "Cost after epoch 4100: 2.0387362011877723\n",
      "Cost after epoch 4110: 2.0386837898307113\n",
      "Cost after epoch 4120: 2.0386315646838273\n",
      "Cost after epoch 4130: 2.038579424942149\n",
      "Cost after epoch 4140: 2.0385273780636695\n",
      "Cost after epoch 4150: 2.0384754315193288\n",
      "Cost after epoch 4160: 2.0384245366761156\n",
      "Cost after epoch 4170: 2.0383738574837094\n",
      "Cost after epoch 4180: 2.0383232954916912\n",
      "Cost after epoch 4190: 2.0382728573926765\n",
      "Cost after epoch 4200: 2.0382225498854796\n",
      "Cost after epoch 4210: 2.038173292804881\n",
      "Cost after epoch 4220: 2.0381242755944533\n",
      "Cost after epoch 4230: 2.0380754024453522\n",
      "Cost after epoch 4240: 2.0380266793240533\n",
      "Cost after epoch 4250: 2.0379781121984197\n",
      "Cost after epoch 4260: 2.0379305877361853\n",
      "Cost after epoch 4270: 2.0378833225892916\n",
      "Cost after epoch 4280: 2.037836223828531\n",
      "Cost after epoch 4290: 2.037789296738192\n",
      "Cost after epoch 4300: 2.037742546600655\n",
      "Cost after epoch 4310: 2.037696825719357\n",
      "Cost after epoch 4320: 2.0376513792714426\n",
      "Cost after epoch 4330: 2.0376061173994917\n",
      "Cost after epoch 4340: 2.0375610447522785\n",
      "Cost after epoch 4350: 2.0375161659745715\n",
      "Cost after epoch 4360: 2.0374722981990137\n",
      "Cost after epoch 4370: 2.037428716085539\n",
      "Cost after epoch 4380: 2.0373853329981175\n",
      "Cost after epoch 4390: 2.0373421529978364\n",
      "Cost after epoch 4400: 2.0372991801405798\n",
      "Cost after epoch 4410: 2.037257195927682\n",
      "Cost after epoch 4420: 2.0372155051377803\n",
      "Cost after epoch 4430: 2.0371740244668413\n",
      "Cost after epoch 4440: 2.037132757436007\n",
      "Cost after epoch 4450: 2.037091707560697\n",
      "Cost after epoch 4460: 2.037051620554096\n",
      "Cost after epoch 4470: 2.0370118316639747\n",
      "Cost after epoch 4480: 2.0369722609935295\n",
      "Cost after epoch 4490: 2.0369329115706853\n",
      "Cost after epoch 4500: 2.0368937864176146\n",
      "Cost after epoch 4510: 2.0368555955666916\n",
      "Cost after epoch 4520: 2.03681770483412\n",
      "Cost after epoch 4530: 2.0367800377738003\n",
      "Cost after epoch 4540: 2.0367425969654662\n",
      "Cost after epoch 4550: 2.0367053849834025\n",
      "Cost after epoch 4560: 2.036669076514183\n",
      "Cost after epoch 4570: 2.036633067822854\n",
      "Cost after epoch 4580: 2.036597285929256\n",
      "Cost after epoch 4590: 2.036561733007768\n",
      "Cost after epoch 4600: 2.0365264112278543\n",
      "Cost after epoch 4610: 2.0364919604594807\n",
      "Cost after epoch 4620: 2.0364578071080035\n",
      "Cost after epoch 4630: 2.0364238816495783\n",
      "Cost after epoch 4640: 2.0363901858935765\n",
      "Cost after epoch 4650: 2.03635672164512\n",
      "Cost after epoch 4660: 2.036324094650492\n",
      "Cost after epoch 4670: 2.0362917609854456\n",
      "Cost after epoch 4680: 2.0362596545511766\n",
      "Cost after epoch 4690: 2.0362277768297234\n",
      "Cost after epoch 4700: 2.036196129299613\n",
      "Cost after epoch 4710: 2.036165284412566\n",
      "Cost after epoch 4720: 2.036134727308624\n",
      "Cost after epoch 4730: 2.036104395264102\n",
      "Cost after epoch 4740: 2.036074289468637\n",
      "Cost after epoch 4750: 2.036044411109112\n",
      "Cost after epoch 4760: 2.0360153002833017\n",
      "Cost after epoch 4770: 2.0359864704760664\n",
      "Cost after epoch 4780: 2.0359578622730443\n",
      "Cost after epoch 4790: 2.0359294766036737\n",
      "Cost after epoch 4800: 2.035901314395379\n",
      "Cost after epoch 4810: 2.0358738844220405\n",
      "Cost after epoch 4820: 2.035846727701055\n",
      "Cost after epoch 4830: 2.0358197880482933\n",
      "Cost after epoch 4840: 2.035793066162528\n",
      "Cost after epoch 4850: 2.035766562741229\n",
      "Cost after epoch 4860: 2.0357407563342655\n",
      "Cost after epoch 4870: 2.035715214604916\n",
      "Cost after epoch 4880: 2.0356898845092086\n",
      "Cost after epoch 4890: 2.035664766542236\n",
      "Cost after epoch 4900: 2.0356398611984488\n",
      "Cost after epoch 4910: 2.0356156179561307\n",
      "Cost after epoch 4920: 2.035591630179794\n",
      "Cost after epoch 4930: 2.035567847866686\n",
      "Cost after epoch 4940: 2.0355442713327863\n",
      "Cost after epoch 4950: 2.035520900894044\n",
      "Cost after epoch 4960: 2.035498158146914\n",
      "Cost after epoch 4970: 2.03547566116902\n",
      "Cost after epoch 4980: 2.035453362892917\n",
      "Cost after epoch 4990: 2.0354312634777827\n",
      "training time: 0:00:00.693392\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAfa0lEQVR4nO3deXRcZ5nn8e9TVVosS7JsSd5kW7JjGy+JYydKcHAIBNINpHsGOoSlD4Stmcw00J3M0D3Q6T5Md9N9ZoDpDIelyQmEAHMCBEgCNJATMiFpZyEmsuM1wrHj2PEix7Ll3dZW9cwf95YlK5JcklV1a/l9zqlTt956q+p5Zfn+dJd6r7k7IiJSumJRFyAiItFSEIiIlDgFgYhIiVMQiIiUOAWBiEiJS0RdwFg1NDR4S0tL1GWIiBSU9evXH3b3xuGeK7ggaGlpoa2tLeoyREQKipntGek57RoSESlxCgIRkRKnIBARKXEKAhGREqcgEBEpcQoCEZESpyAQESlxJRMELx8+zZ2/3s7aFzs52d0XdTkiInmj4L5QNl5b9h/na4/vJOUQM7hsTh3vbZ3DTavmMKk8HnV5IiKRsUK7ME1ra6uP95vFp3r62fjKMZ7b3cUj2w7y+4MnmTN1El+8eQVvuKRhgisVEckfZrbe3VuHfa6UgmAwd+e3Lx3h7362lVeOnOGLN6/gpivmTECFIiL5Z7QgKJljBEOZGW9Y2MDPPrmGq+dP469/spknd3RGXZaISM6VbBCk1VSWcfeHWlnYWM1/vX8Tx870Rl2SiEhOlXwQAFRXJLjzfZdz7EwvX3pke9TliIjklIIgtHz2FP706nnc/9xe9nadibocEZGcURAM8snrFxIz41tP7oq6FBGRnFEQDDJzSiU3XjaTBzfs53RPf9TliIjkhIJgiA+ubuZkTz+/3NIRdSkiIjmhIBjiyuapzJk6iYcVBCJSIhQEQ5gZ77h0Jk/vPMIJzUkkIiVAQTCMt186i95kiie26wtmIlL8FATDWDm3jtrKBM/sPBx1KSIiWacgGEY8ZqxeUM/TLykIRKT4KQhG8IZL6tnbdVZfLhORoqcgGME14bTU617uirgSEZHsyloQmNlcM3vczNrNbJuZ3TZMnzeb2XEz2xjePpetesZq0fRqqisSbNp7LOpSRESyKptXKOsHPu3uG8ysBlhvZo+6+wtD+j3p7n+cxTrGJRYzVsyZwqZ9CgIRKW5Z2yJw9w533xAunwTagaZsfV42XD63jvaOE3T3JaMuRUQka3JyjMDMWoBVwLphnr7GzDaZ2cNmtnyE199qZm1m1tbZmbtz+y+fU0df0nmh40TOPlNEJNeyHgRmVg08ANzu7kPXqBuAZne/HPgq8NPh3sPd73b3VndvbWxszG7Bg6ycWweg4wQiUtSyGgRmVkYQAve5+4NDn3f3E+5+Klz+FVBmZnlzFfmZUyppqC6nXVsEIlLEsnnWkAH3AO3ufucIfWaG/TCzq8N6jmSrpvFYPKOG7a+eiroMEZGsyeZZQ2uAW4AtZrYxbLsDmAfg7ncBNwN/bmb9wFng/e7uWaxpzBbPqOFHbXtJpZxYzKIuR0RkwmUtCNz9KWDUNae7fw34WrZqmAhLZtZwpjfJ/mNnmTutKupyREQmnL5ZfAGLZ9YA8PuDJyOuREQkOxQEF7BoejUAL76qIBCR4qQguICayjKa6iaxXVsEIlKkFAQZWNA4mZcPn466DBGRrFAQZGBBw2R2Hz5Nnp3QJCIyIRQEGWhpmMzJnn4On+qNuhQRkQmnIMjA/IbJAOw+ot1DIlJ8FAQZWNAQnDn0cqeCQESKj4IgA7PrKimLG7t0wFhEipCCIAOJeIx506rYrSAQkSKkIMjQ/AadQioixUlBkKH5DZPZfeQ0qZROIRWR4qIgyFBLw2R6+lN0nOiOuhQRkQmlIMjQ/PrgFNI92j0kIkVGQZCh5nPfJTgTcSUiIhNLQZChWbWVlCdi7NGXykSkyCgIMhSLWXAKqYJARIqMgmAMWuqr2KNdQyJSZBQEY9BcH5xCqllIRaSYKAjGoKW+iu6+FIdO9kRdiojIhFEQjEFzeAqpppoQkWKiIBiDlvR3CXScQESKiIJgDGbXVZKImc4cEpGioiAYg/QspNoiEJFioiAYo+Z6fZdARIqLgmCMmut1IXsRKS4KgjFqqa/idG9SF7IXkaKhIBij9ORzmnNIRIqFgmCM0qeQahZSESkWWQsCM5trZo+bWbuZbTOz20bpe5WZJc3s5mzVM1Ga6iYRj5m2CESkaCSy+N79wKfdfYOZ1QDrzexRd39hcCcziwNfAB7JYi0TpjwRo6lukrYIRKRoZG2LwN073H1DuHwSaAeahun6F8ADwKFs1TLRmuurtEUgIkUjJ8cIzKwFWAWsG9LeBPwJcNcFXn+rmbWZWVtnZ2e2ysxYS/1kXtYppCJSJLIeBGZWTfAX/+3ufmLI018GPuPuydHew93vdvdWd29tbGzMVqkZa66v4mR3P8fO9EVdiojIRcvmMQLMrIwgBO5z9weH6dIK/NDMABqAG82s391/ms26LtbAmUOnmTq5POJqREQuTjbPGjLgHqDd3e8cro+7z3f3FndvAX4CfCLfQwCgpaEK0CykIlIcsrlFsAa4BdhiZhvDtjuAeQDuPupxgXw2Z2oVZmjOIREpClkLAnd/CrAx9P9ItmqZaJVlcWZPmaQtAhEpCvpm8ThpFlIRKRYKgnFqaZisLQIRKQoKgnFqqa+i63Qvx8/qFFIRKWwKgnFqrtcspCJSHBQE46RZSEWkWCgIxmnetPC7BIe1RSAihU1BME6TyuPMrK3UFoGIFDwFwUXQLKQiUgwUBBchPQupiEghUxBchMUzazhyupdDJ7ujLkVEZNwUBBfh0tm1AGw7MHR2bRGRwqEguAjL0kGw/3jElYiIjJ+C4CLUVJbRXF+lLQIRKWgKgou0fHatgkBECpqC4CItnz2FV7rOcFyXrRSRAqUguEir5tUBsOGVoxFXIiIyPgqCi7Rybh2JmPHc7q6oSxERGRcFwUWqKk+wvGkKbbu1RSAihUlBMAGuap7Kxn3H6OlPRl2KiMiYKQgmQGvLNHr7U2zZp+8TiEjhURBMgKtapgLw7K4jEVciIjJ2CoIJUF9dwfLZtazdcTjqUkRExkxBMEHeuKiRDXuOcqqnP+pSRETGREEwQa5b1EB/ylmn3UMiUmAUBBPkypapVJbFeFK7h0SkwCgIJkhFIs7qBfWs3dEZdSkiImOiIJhAb1zUyK7O0+w/djbqUkREMpZREJjZezJpK3VvXNQAwNPaPSQiBSTTLYK/ybCtpC2aXk1jTQVP7VQQiEjhSIz2pJm9A7gRaDKzrwx6qhbQeZJDmBnXLmxg7YudpFJOLGZRlyQickEX2iI4ALQB3cD6QbefA28b7YVmNtfMHjezdjPbZma3DdPnnWa22cw2mlmbmV07vmHkj2sXNnDkdC/tB3WxGhEpDKNuEbj7JmCTmX3f3fsAzGwqMNfdLzTdZj/waXffYGY1wHoze9TdXxjU5zHg5+7uZrYC+BGwZNyjyQNrFobHCXYeZvnsKRFXIyJyYZkeI3jUzGrNbBqwCbjXzO4c7QXu3uHuG8Llk0A70DSkzyl39/DhZMApcDOnVLJoerW+TyAiBSPTIJji7ieAm4B73f1K4IZMP8TMWoBVwLphnvsTM/s98EvgYyO8/tZw11FbZ2f+n6e/ZmEDz+3uortP01KLSP7LNAgSZjYLeC/wi7F8gJlVAw8At4dhch53f8jdlwDvAj4/3Hu4+93u3ururY2NjWP5+EisWdhAd1+KjXuPRV2KiMgFZRoE/wg8Arzk7s+Z2QJgx4VeZGZlBCFwn7s/OFpfd18LXGJmDRnWlLfS01Kv36OrlolI/hv1YHGau/8Y+PGgx7uAd4/2GjMz4B6g3d2HPZ5gZgsJwsXN7AqgHCj4WdvqqspZOL1a1zEWkYKQURCY2Rzgq8AaggO6TwG3ufu+UV62BrgF2GJmG8O2O4B5AO5+F0GYfMjM+oCzwPsGHTwuaFe1TOWXmzv0fQIRyXsZBQFwL/B9ID2txAfDtj8Y6QXu/hQw6hrQ3b8AfCHDGgrKlc3T+MHv9rLj0CleN7Mm6nJEREaU6TGCRne/1937w9t3gPw/ahuh1ubgOIF2D4lIvss0CA6b2QfNLB7ePkgR7MvPpub6KhqqK3TAWETyXqZB8DGCU0cPAh3AzcBHs1VUMTAzWpun0rZHWwQikt8yDYLPAx9290Z3n04QDH+ftaqKxMp5deztOsvR071RlyIiMqJMg2DF4LmF3L2L4JvCMorLmoK5hrbsPx5xJSIiI8s0CGLhZHMAhHMOZXrGUcm6dLaCQETyX6Yr838BnjGznxB8j+C9wD9nraoiMaWqjJb6KrbsUxCISP7K9JvF3zOzNuAtBN8NuGnIdNIygsvm1LFBZw6JSB7LePdOuOLXyn+MLmuq5d82HeDIqR7qqyuiLkdE5DUyPUYg43RZUx2g4wQikr8UBFl2aVMtgI4TiEjeUhBkWU1lGfMbJrP1gIJARPKTgiAHls6qob3jZNRliIgMS0GQA0tn1vJK1xlOdvdFXYqIyGsoCHJg6azgOMH2g9oqEJH8oyDIgaWzgyBo73jNJZtFRCKnIMiB2VMqqa1M8IKOE4hIHlIQ5ICZsWx2rbYIRCQvKQhyZOmsWrYfPEkyVRSXZBaRIqIgyJGls2o525dkz5HTUZciInIeBUGOLJuVPmCs4wQikl8UBDmycHo18ZjpOIGI5B0FQY5UlsW5pHGygkBE8o6CIIeWzqrlBQWBiOQZBUEOLZ1VS8fxbo6d0cXsRSR/KAhyKD3VhLYKRCSfKAhyaOmsGkBnDolIflEQ5ND0mkoaqst1wFhE8oqCIMeWztJUEyKSX7IWBGY218weN7N2M9tmZrcN0+cDZrY5vD1jZpdnq558sWxWLTtePUVfMhV1KSIiQHa3CPqBT7v7UmA18EkzWzakz8vAm9x9BfB54O4s1pMXls2upTeZYserp6IuRUQEyGIQuHuHu28Il08C7UDTkD7PuPvR8OGzwJxs1ZMvls/WmUMikl9ycozAzFqAVcC6Ubr9GfDwCK+/1czazKyts7Nz4gvMofkN1Uwqi7NNF7MXkTyR9SAws2rgAeB2dx/2z2Azu54gCD4z3PPufre7t7p7a2NjY/aKzYF4zFgyq4ZtB7RFICL5IatBYGZlBCFwn7s/OEKfFcC3gHe6+5Fs1pMvls+upf3ACdx1bQIRiV42zxoy4B6g3d3vHKHPPOBB4BZ3fzFbteSbZbOmcLKnn71dZ6MuRUSERBbfew1wC7DFzDaGbXcA8wDc/S7gc0A98K9BbtDv7q1ZrCkvpA8YbztwnHn1VRFXIyKlLmtB4O5PAXaBPh8HPp6tGvLV62bWEI8ZL3Sc4B2XzYq6HBEpcfpmcQTS1ybQAWMRyQcKgogsnz1Fp5CKSF5QEERk+exaXj3Rw+FTPVGXIiIlTkEQkWXpbxhr95CIRExBEJFls9JnDikIRCRaCoKI1FWV01Q3SccJRCRyCoIILZ9dq11DIhI5BUGELmuawq7Dpzl+ti/qUkSkhCkIIrRq3lQANu87FnElIlLKFAQRWjF3Cmaw8RUFgYhER0EQodrKMhY2VvP8XgWBiERHQRCxVfPqeP6Vo5qSWkQioyCI2Kp5Uzl6po89R85EXYqIlCgFQcRWzq0D4Pm9Ry/QU0QkOxQEEVs8o4aq8rgOGItIZBQEEYvHjMvn1OmAsYhERkGQB65oruOFAyc409sfdSkiUoIUBHng6vn19Kec9Xt0nEBEck9BkAdam6cSjxnP7joSdSkiUoIUBHlgckWCFXOm8OyurqhLEZESpCDIE6+fX8/mfcd0nEBEck5BkCdWL5hGX9LZsEdnD4lIbikI8kRryzQdJxCRSCgI8kR1RYKVc+tYu6Mz6lJEpMQoCPLImxc3snnfcTpP9kRdioiUEAVBHrl+yXQA1r6orQIRyR0FQR5ZNquWxpoKnlAQiEgOKQjySCxmvGlxI2tf7KQ/mYq6HBEpEQqCPHP966Zz/GwfGzUJnYjkSNaCwMzmmtnjZtZuZtvM7LZh+iwxs9+aWY+Z/VW2aikk1y1uoDwe4+GtB6MuRURKRDa3CPqBT7v7UmA18EkzWzakTxfwl8D/zmIdBaWmsozrFjfyqy0dpFK6fKWIZF/WgsDdO9x9Q7h8EmgHmob0OeTuzwF92aqjEP3Ripl0HO/WNQpEJCdycozAzFqAVcC6cb7+VjNrM7O2zs7iP6PmhqUzKE/E+OXmjqhLEZESkPUgMLNq4AHgdnc/MZ73cPe73b3V3VsbGxsntsA8VFNZxpsWN/KLzQd09pCIZF1Wg8DMyghC4D53fzCbn1Vs3nPlHA6d7OGJ7cW/BSQi0crmWUMG3AO0u/ud2fqcYnX9kuk01lRwf9veqEsRkSKXyOJ7rwFuAbaY2caw7Q5gHoC732VmM4E2oBZImdntwLLx7kIqJmXxGO++Yg7ffHIXh050M722MuqSRKRIZS0I3P0pwC7Q5yAwJ1s1FLr3XTWXu/79Je5/bi9/8dZFUZcjIkVK3yzOY/MbJvOmxY1897d76O5LRl2OiBQpBUGe+8/XLeDwqR4een5/1KWISJFSEOS5ay6p59KmWr65dpdOJRWRrFAQ5Dkz41PXL2LX4dM8uEFbBSIy8RQEBeBty2ewcm4d/+f/vahjBSIy4RQEBcDM+Mzbl9BxvJt7n94ddTkiUmQUBAXimkvquWHpDL7y2A72dp2JuhwRKSIKggLyj+9cTszgjoe24K4pqkVkYigICsjsukl85h1LeHLHYb6tXUQiMkEUBAXmltXN3LB0Bv/r4XZdzlJEJoSCoMCYGf/ynsuZXlPJf/pem44XiMhFUxAUoClVZXzno1fR25/ilnvWcfB4d9QliUgBUxAUqEUzavj2R67i8Kle3v2NZ9h56FTUJYlIgVIQFLArm6fyw1tX09Of5KZ/fZpfbdGlLUVk7BQEBe7Spik89Ik1zG+s5hP3beBT39/AgWNnoy5LRAqIFdr56K2trd7W1hZ1GXmnL5ni64/v5BtPvATAzVfO4SNvaGHRjJqc1uHunOjup+t0L12nezhyqpejZ3o51ZOkuy/J2d4kZ/uCW3e43Jd0kqkUSYdUykmmnKSH9ykPvjNhhgExCw6YB8sGFrZhxGLBvZ3XJ1iOGYCFj4PXmp3fP3ifwe878Dnn3nPQ56XfJxXWOnAfjiOD9lQKku6k0o8dPPw5ptxxJ2g773Fw7xC+Jnif9M/fzIjHgpsZxC29bMRjQc2xsC0WjqEsHqM8EaMsbpQn4pTFjYpELGiPxyhLBPfl4f3gfuWJGJVl8eA2eLksXE7EqSiLUZGIEVy4UKJgZuvdvXXY5xQExWXf0TN89bGdPLRxP739KZbMrOGGpTO4snkqK+ZMob66Ykzvl0o5x8/20XWml67TvRw51cuR0z10nerlyOnecIWfXu6h63QvfcmRf6diBlXlCSrL4kwqj1GZiFMWjwUrpZgRN86txIIVVXCDYKWXvndnYKVIsAI8/3HYNqh/eoUarER94PGg/qn0c+GKNfWa9z3/cSrlYd127v7cCnhIeyxc8abHlX7eBo05+BnZeQE2EFJh2A0KqPSKPB1c6ZpTYfAMDplkaiA4goDlXOj2J1P0JZ3e/hR9yRS9ydTAcn/43EXOfmsGFYmBcKgsi1GRvn9NkJwfLhWDw+VcsMSpSAQBU1E2wnIYVgogBUFJOnKqh59uPMAj2w7StruLVPjPXFOZoKluEo01Fef+oyViRm9/ip7+YAVwsruPY2f6OHqml+Nn+xjpV6SmIsG06nKmTS6nfnJ4X11xbjlor2BadTnVFQkmlek/ZSFz93OB0Nc/EBY9/Sl6+pN096Xo6UvSHS539w26H/x8ur0/OaTP8M/39KdG/B3MRDqAyuOvDYn0lsq5UBnyfHki3FKKGWWJGIlYsAWUiAVbRWXxYKspETfKw/uy+MBziViM8oQF/dPvc16/2Lk/ALJNQVDiTvX0s3X/cbbuP86+o2fZf+wsnSd7wv9wSZLu4WZ/8ItfW5mgrqqcukllTK0qo65qYMUerOyD+4pEPOqhSQlwD8JnIEiC4EkHUE/fQBj1nAumoG/PoKB6Tb++Ia/pS54XbD1hGI22hTsRzCARbhEmYrHw3gbu4wNbmn969Tw+/sYF4/yckYMgmxevlzxRXZFg9YJ6Vi+oj7oUkTEzs/Av9jhMKsv557s7/SmnP9waSu9G60sGu876U8Eutf7UQFtfMr27baDv0Nf3hm3JVPDaZMoH3aeC++T57Q1j3LWbKQWBiMgozCzc1QOTKM6tYJ0+KiJS4hQEIiIlTkEgIlLiFAQiIiVOQSAiUuIUBCIiJU5BICJS4hQEIiIlruCmmDCzTmDPOF/eAByewHIKgcZcGjTm0nAxY25298bhnii4ILgYZtY20lwbxUpjLg0ac2nI1pi1a0hEpMQpCERESlypBcHdURcQAY25NGjMpSErYy6pYwQiIvJapbZFICIiQygIRERKXMkEgZm93cy2m9lOM/ts1PVcDDP7tpkdMrOtg9qmmdmjZrYjvJ8atpuZfSUc92Yzu2LQaz4c9t9hZh+OYiyZMLO5Zva4mbWb2TYzuy1sL+YxV5rZ78xsUzjmfwjb55vZurD++82sPGyvCB/vDJ9vGfRefxO2bzezt0UzosyZWdzMnjezX4SPi3rMZrbbzLaY2UYzawvbcvu77e5FfwPiwEvAAqAc2AQsi7quixjPdcAVwNZBbV8EPhsufxb4Qrh8I/AwYMBqYF3YPg3YFd5PDZenRj22EcY7C7giXK4BXgSWFfmYDagOl8uAdeFYfgS8P2y/C/jzcPkTwF3h8vuB+8PlZeHvewUwP/x/EI96fBcY+38Dvg/8Inxc1GMGdgMNQ9py+rtdKlsEVwM73X2Xu/cCPwTeGXFN4+bua4GuIc3vBL4bLn8XeNeg9u954FmgzsxmAW8DHnX3Lnc/CjwKvD371Y+du3e4+4Zw+STQDjRR3GN2dz8VPiwLbw68BfhJ2D50zOmfxU+At5qZhe0/dPced38Z2Enw/yEvmdkc4I+Ab4WPjSIf8why+rtdKkHQBOwd9Hhf2FZMZrh7BwQrTmB62D7S2AvyZxJu/q8i+Au5qMcc7iLZCBwi+I/9EnDM3fvDLoPrPze28PnjQD0FNmbgy8B/B1Lh43qKf8wO/NrM1pvZrWFbTn+3S+Xi9TZMW6mcNzvS2AvuZ2Jm1cADwO3ufiL442/4rsO0FdyY3T0JrDSzOuAhYOlw3cL7gh+zmf0xcMjd15vZm9PNw3QtmjGH1rj7ATObDjxqZr8fpW9WxlwqWwT7gLmDHs8BDkRUS7a8Gm4iEt4fCttHGntB/UzMrIwgBO5z9wfD5qIec5q7HwOeINgnXGdm6T/gBtd/bmzh81MIdh8W0pjXAP/RzHYT7L59C8EWQjGPGXc/EN4fIgj8q8nx73apBMFzwKLw7INyggNLP4+4pon2cyB9psCHgZ8Nav9QeLbBauB4uKn5CPCHZjY1PCPhD8O2vBPu970HaHf3Owc9Vcxjbgy3BDCzScANBMdGHgduDrsNHXP6Z3Ez8BsPjiL+HHh/eIbNfGAR8LvcjGJs3P1v3H2Ou7cQ/B/9jbt/gCIes5lNNrOa9DLB7+RWcv27HfUR81zdCI62v0iwn/Vvo67nIsfyA6AD6CP4S+DPCPaNPgbsCO+nhX0N+Ho47i1A66D3+RjBgbSdwEejHtco472WYDN3M7AxvN1Y5GNeATwfjnkr8LmwfQHBSm0n8GOgImyvDB/vDJ9fMOi9/jb8WWwH3hH12DIc/5sZOGuoaMccjm1TeNuWXjfl+ndbU0yIiJS4Utk1JCIiI1AQiIiUOAWBiEiJUxCIiJQ4BYGISIlTEEjRM7P/aWZvNrN32Rhnng3P518Xzob5xmzVOMJnn7pwL5GLpyCQUvB6grmJ3gQ8OcbXvhX4vbuvcvexvlakICgIpGiZ2ZfMbDNwFfBb4OPAN8zsc8P0bTazx8I53h8zs3lmtpJgOuAbw7niJw15zZVm9u/hZGGPDJoS4Akz+7KZPWNmW83s6rB9mpn9NPyMZ81sRdhebWb3hnPSbzazdw/6jH+24JoEz5rZjLDtPeH7bjKztdn56UlJifqbdbrpls0bwbwtXyWYxvnpUfr9G/DhcPljwE/D5Y8AXxumfxnwDNAYPn4f8O1w+Qngm+HydYTXjQjr+B/h8luAjeHyF4AvD3rvqeG9A/8hXP4i8Hfh8hagKVyui/pnrFvh30pl9lEpXasIpqRYArwwSr9rgJvC5f9LsOIdzeuASwlmi4Tg4kcdg57/AQTXjjCz2nDeoGuBd4ftvzGzejObQjCP0PvTL/RgPnmAXuAX4fJ64A/C5aeB75jZj4D0BHwi46YgkKIU7tb5DsEsjIeBqqDZNgLXuPvZC7zFheZeMWCbu1+T4etHmyrYRvi8PndPtycJ/7+6+38xs9cTXMBlo5mtdPcjF6hXZEQ6RiBFyd03uvtKBi5r+Rvgbe6+coQQeIaBv8o/ADx1gY/YDjSa2TUQTJNtZssHPf++sP1aghkijwNrw/cmnG//sLufAH4NfCr9wnD2yBGZ2SXuvs7dP0cQcnNH6y9yIdoikKJlZo3AUXdPmdkSdx9t19BfAt82s78GOoGPjvbe7t5rZjcDXwl37yQI5s7fFnY5ambPALUExxwA/h64NzyAfYaBaYb/Cfi6mW0l+Mv/Hxh9l8+XzGwRwZbEYwQzV4qMm2YfFZlgZvYE8Ffu3hZ1LSKZ0K4hEZESpy0CEZESpy0CEZESpyAQESlxCgIRkRKnIBARKXEKAhGREvf/ASTu1SKrEkDEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "paras = skipgram_model_training(X, Y, vocab_size, 50, 0.05, 5000, batch_size=128, parameters=None, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
